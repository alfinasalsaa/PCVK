{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfinasalsaa/PCVK/blob/main/Kuis2_PCVK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kelompok 1 :**\n",
        "\n",
        "\n",
        "# **1.   Alfina Salsabilla 2141720044**\n",
        "# **2.   M. Rafi Prabowo 2141720239**\n",
        "# **3. Syaikhul Syafwan R 2141720105**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bI0XyLP9b3mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap4jcPZ30Z6U",
        "outputId": "beb9e902-ed39-4977-81c1-68c3ad1b82a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AmYgM8OSbyEf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import File**"
      ],
      "metadata": {
        "id": "Sv7x5_Eikeh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pathProcessing (path):\n",
        "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.gif']  # Add more extensions if needed\n",
        "    image_paths = []\n",
        "\n",
        "    for ext in image_extensions:\n",
        "        image_paths.extend(glob.glob(os.path.join(path, ext)))\n",
        "\n",
        "    return image_paths"
      ],
      "metadata": {
        "id": "q8olTa4sjZzJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readImage (paths):\n",
        "    images=[]\n",
        "\n",
        "    for path in paths:\n",
        "        image= cv2.imread(path,0)\n",
        "        if image is not None:\n",
        "            images.append(image)\n",
        "        else:\n",
        "            print(f\"Failed to read image at path: {path}\")\n",
        "\n",
        "    return images"
      ],
      "metadata": {
        "id": "O_4GAOGJkmZ0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(images, num_cols=3):\n",
        "    num_images = len(images)\n",
        "    num_rows = (num_images + num_cols - 1) // num_cols\n",
        "\n",
        "    plt.figure(figsize=(10, 8))  # Atur ukuran figur sesuai kebutuhan\n",
        "\n",
        "    for i, image in enumerate(images, 1):\n",
        "        plt.subplot(num_rows, num_cols, i)\n",
        "        plt.imshow(image, cmap='gray')  # Menggunakan cmap='gray' jika gambar dalam mode grayscale\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DW5hV5YVkqaI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre Procesing**"
      ],
      "metadata": {
        "id": "PZiLYN47llx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussianBlur(images):\n",
        "    blurredImages=[]\n",
        "    for image in images:\n",
        "        kernel_size = (5, 5)\n",
        "        sigma_x = 0\n",
        "        blurredImage = cv2.GaussianBlur(image, kernel_size, sigma_x)\n",
        "        blurredImages.append(blurredImage)\n",
        "\n",
        "    return blurredImages"
      ],
      "metadata": {
        "id": "SWAku0zjlvGp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def histogramEqualizing(images):\n",
        "    equalizedImages=[]\n",
        "    for image in images:\n",
        "        equalizedImage = cv2.equalizeHist(image)\n",
        "        equalizedImages.append(equalizedImage)\n",
        "\n",
        "    return equalizedImages"
      ],
      "metadata": {
        "id": "Nw5lZ8IGlwuh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contourDetect(images):\n",
        "    cleanImages=[]\n",
        "    for image in images:\n",
        "        _, thresh = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        # Find contours\n",
        "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # Choose the contour with the largest area\n",
        "        largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "        # Create a mask to extract the necessary part\n",
        "        mask = np.zeros_like(image)\n",
        "        cv2.drawContours(mask, [largest_contour], -1, (255), thickness=cv2.FILLED)\n",
        "\n",
        "        result = cv2.bitwise_and(image, image, mask=mask)\n",
        "\n",
        "        cleanImages.append(result)\n",
        "\n",
        "    return cleanImages"
      ],
      "metadata": {
        "id": "iMQlEkDqlz6Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def erode(images):\n",
        "\n",
        "    erodedImages=[]\n",
        "    for image in images:\n",
        "        kernel = (5, 5)\n",
        "        erodedImage  = cv2.erode(image, kernel, iterations=1)\n",
        "        erodedImages.append(erodedImage)\n",
        "\n",
        "    return erodedImages"
      ],
      "metadata": {
        "id": "6MN2wQodl2t9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adaptiveThresholding(images):\n",
        "    thresholded=[]\n",
        "    for image in images:\n",
        "        normalizedImage = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "        adaptiveThreshold = cv2.adaptiveThreshold(normalizedImage, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "        thresholded.append(adaptiveThreshold)\n",
        "    return thresholded"
      ],
      "metadata": {
        "id": "G1GOXkKAl-r_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def laplacianEdgeDetect(images):\n",
        "    edgeDetected=[]\n",
        "    for image in images :\n",
        "        laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
        "        edgeDetected.append(laplacian)\n",
        "\n",
        "    return edgeDetected"
      ],
      "metadata": {
        "id": "WwrXREw3mCzE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sobelEdgeDetect(images):\n",
        "    edgeDetected=[]\n",
        "    for image in images :\n",
        "        normalizedImage = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "        sobel_x = cv2.Sobel(normalizedImage, cv2.CV_64F, 1, 0, ksize=5)\n",
        "        sobel_y = cv2.Sobel(normalizedImage, cv2.CV_64F, 0, 1, ksize=5)\n",
        "        sobelCombined = cv2.magnitude(sobel_x, sobel_y)\n",
        "        edgeDetected.append(sobelCombined)\n",
        "    return sobelCombined"
      ],
      "metadata": {
        "id": "vKTdvKWRmOKq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def otsuThresholding(images):\n",
        "    thresholded=[]\n",
        "    for image in images:\n",
        "        normalizedImage = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "        ret, otsu_threshold = cv2.threshold(normalizedImage, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        thresholded.append(otsu_threshold)\n",
        "    return thresholded"
      ],
      "metadata": {
        "id": "IRuQoX7jMqN2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bitwiseNot(images):\n",
        "    flippedImage=[]\n",
        "    for image in images :\n",
        "        bitwise_not_image = cv2.bitwise_not(image)\n",
        "        flippedImage.append(bitwise_not_image)\n",
        "    return flippedImage"
      ],
      "metadata": {
        "id": "HZcKspWlMtWa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contour(images, templates, originalImages, threshold=0.8):\n",
        "    matched_regions = []\n",
        "    count = 0\n",
        "    for image in images:\n",
        "        found = False\n",
        "        for template in templates:\n",
        "            if not found:\n",
        "                h, w = template.shape[:2]\n",
        "                if h > image.shape[0] or w > image.shape[1]:\n",
        "                    template = cv2.resize(template, (image.shape[1], image.shape[0]))\n",
        "\n",
        "                result = cv2.matchTemplate(image, template, cv2.TM_CCOEFF)\n",
        "                min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
        "                if max_val >= threshold:\n",
        "                    found = True\n",
        "                    top_left = max_loc\n",
        "                    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
        "\n",
        "                    # Membuat salinan kosong dari gambar\n",
        "                    matched_region = np.zeros_like(originalImages[count])\n",
        "\n",
        "                    # Menambahkan kotak hitam di area di mana template cocok\n",
        "                    cv2.rectangle(matched_region, top_left, bottom_right, (225, 225, 225), 4)\n",
        "                    matched_regions.append(matched_region)\n",
        "                    break  # Hentikan pencarian template setelah yang pertama ditemukan\n",
        "        count += 1\n",
        "\n",
        "    return matched_regions"
      ],
      "metadata": {
        "id": "DosLpFbJMwcq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matchFeature(images, templates,originalImages, threshold=0.8):\n",
        "    matched_regions = []\n",
        "    count=0\n",
        "    for image in images:\n",
        "        found = False\n",
        "        for template in templates:\n",
        "            if not found:\n",
        "                h, w = template.shape[:2]\n",
        "                if h > image.shape[0] or w > image.shape[1]:\n",
        "                    template = cv2.resize(template, (image.shape[1], image.shape[0]))\n",
        "\n",
        "                result = cv2.matchTemplate(image, template, cv2.TM_CCOEFF)\n",
        "                min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
        "                if max_val >= threshold:\n",
        "                    found = True\n",
        "                    top_left = max_loc\n",
        "                    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
        "                    matched_regions.append(originalImages[count][top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]])\n",
        "                    # Menyimpan potongan gambar yang sesuai dengan template yang ditemukan\n",
        "                    break  # Hentikan pencarian template setelah yang pertama ditemukan\n",
        "        count+=1\n",
        "\n",
        "    return matched_regions"
      ],
      "metadata": {
        "id": "Vcdr7byGM0Yf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def faceDet(images):\n",
        "    faces = []\n",
        "    for image in images:\n",
        "\n",
        "        mukaTamvan = cv2.CascadeClassifier('/content/drive/MyDrive/PCVK/Week12/Face Detection/haarcascade_frontalface_alt.xml')\n",
        "        roi_wajah = mukaTamvan.detectMultiScale(image)\n",
        "\n",
        "        for (x, y, w, h) in roi_wajah:\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), (255, 255, 255), 2)  # Garis putih pada wajah\n",
        "\n",
        "        faces.append(image)\n",
        "    return faces"
      ],
      "metadata": {
        "id": "L8xd5M1TM4Y_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_multiple_images(images_list, titles_list):\n",
        "    num_cols = len(images_list)\n",
        "    num_rows = max(len(images) for images in images_list)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))  # Sesuaikan ukuran figur sesuai kebutuhan\n",
        "\n",
        "    for i in range(num_rows):\n",
        "        for j in range(num_cols):\n",
        "            if i < len(images_list[j]):\n",
        "                plt.subplot(num_rows, num_cols, i * num_cols + j + 1)\n",
        "                plt.imshow(images_list[j][i], cmap='gray' if j == 1 else None)  # cmap='gray' untuk tresholded\n",
        "                plt.title(titles_list[j])\n",
        "                plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IKUj9GO5Nb59"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main ():\n",
        "    folderPath= '/content/drive/MyDrive/PCVK/Kuis 2/KTP'\n",
        "    folderMuka= '/content/drive/MyDrive/PCVK/Kuis 2/muka'\n",
        "\n",
        "    featuresPath=pathProcessing(folderMuka)\n",
        "\n",
        "    paths=pathProcessing(folderPath)\n",
        "    features=readImage(featuresPath)\n",
        "\n",
        "    KTP= readImage(paths)\n",
        "    KTP2= readImage(paths)\n",
        "    KTP3= readImage(paths)\n",
        "    KTP4= readImage(paths)\n",
        "\n",
        "\n",
        "    thresholded= otsuThresholding(erode(KTP))\n",
        "\n",
        "    kontour=contour(otsuThresholding(erode(KTP2)),features,KTP2)\n",
        "\n",
        "    matched=matchFeature(otsuThresholding(erode(KTP3)),features,KTP3)\n",
        "\n",
        "    faces_detected = faceDet(matchFeature(otsuThresholding(erode(KTP4)),features,KTP4))\n",
        "\n",
        "    return {\n",
        "        'images': KTP,\n",
        "        'thresholded': thresholded,\n",
        "        'contour': kontour,\n",
        "        'matched': matched,\n",
        "        'faces_detected': faces_detected"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "CXidYNwlNetK",
        "outputId": "fb334154-531a-42bb-a1ff-d26553825f02"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-e09c98175f41>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    'faces_detected': faces_detected\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=main()"
      ],
      "metadata": {
        "id": "x72TOWo9Nzh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_list = [result['images'], result['thresholded'],result['contour'], result['matched'], result['faces_detected']]\n",
        "titles_list = ['Images', 'Thresholded','contour', 'Matched', 'Faces Detected']\n",
        "\n",
        "show_multiple_images(images_list, titles_list)"
      ],
      "metadata": {
        "id": "mwyxNLX_N2ZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}